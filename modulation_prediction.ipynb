{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "IQ_data_file = h5py.File(\"data.hdf5\", \"r\")\n",
    "raw_IQ_train = IQ_data_file[\"train\"][:]\n",
    "raw_IQ_test = IQ_data_file[\"test\"][:]\n",
    "\n",
    "print(\"Raw I/Q training data has shape:\", raw_IQ_train.shape)\n",
    "print(\"Raw I/Q testing data has shape:\", raw_IQ_test.shape)\n",
    "\n",
    "training_labels = pd.read_csv(\"train_labels.csv\").to_numpy()[:,1]\n",
    "\n",
    "print(\"Training label data has shape:\", training_labels.shape)\n",
    "df = pd.DataFrame(training_labels, columns=[\"Modulation Category\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map training label data to integers and create new label vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_PARTITIONS = 4\n",
    "labels = [\"FM\",\"OQPSK\",\"BPSK\",\"8PSK\",\"AM-SSB-SC\",\"4ASK\",\"16PSK\",\"AM-DSB-SC\",\"QPSK\",\"OOK\"]\n",
    "label_to_int, int_to_label = dict(), dict()\n",
    "for i,label in enumerate(labels):\n",
    "    label_to_int[label] = i\n",
    "    int_to_label[i] = label\n",
    "convert_label_to_int = lambda l : label_to_int[l]\n",
    "\n",
    "# Create N_PARTITIONS duplicates since signal data is also being partitioned\n",
    "y_train = np.array(list(map(convert_label_to_int, np.repeat(training_labels, N_PARTITIONS))))\n",
    "\n",
    "print(\"Label-to-integer mappings:\", label_to_int)\n",
    "print(\"Expanded training label data has shape:\", y_train.shape)\n",
    "for i in range(len(labels)):\n",
    "    print(\"Number of examples with label \" + int_to_label[i] + \" is:\",  sum(y_train==i))\n",
    "df = pd.DataFrame(y_train, columns=[\"Modulation Category\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition signals to create smaller signals as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "I_train = raw_IQ_train[:,:,0]\n",
    "Q_train = raw_IQ_train[:,:,1]\n",
    "I_test = raw_IQ_test[:,:,0]\n",
    "Q_test = raw_IQ_test[:,:,1]\n",
    "N_TRAIN = raw_IQ_train.shape[0]\n",
    "N_TEST = raw_IQ_test.shape[0]\n",
    "N_SAMPLES = int(raw_IQ_train.shape[1] / N_PARTITIONS)\n",
    "N_CHANNELS = raw_IQ_train.shape[2]\n",
    "\n",
    "complex_train = np.ndarray(((N_TRAIN*N_PARTITIONS), N_SAMPLES), dtype=np.complex64)\n",
    "complex_test = np.ndarray(((N_TEST*N_PARTITIONS), N_SAMPLES), dtype=np.complex64)\n",
    "\n",
    "if N_PARTITIONS == 1:\n",
    "    complex_train = I_train + 1j*Q_train\n",
    "    complex_test = I_test + 1j*Q_test\n",
    "else:\n",
    "    for n in range(N_TRAIN):\n",
    "        start_slot = N_PARTITIONS*n\n",
    "        for i in range(N_PARTITIONS):\n",
    "            curr_slot = start_slot + i\n",
    "            start_idx = N_SAMPLES*i\n",
    "            end_idx = start_idx + N_SAMPLES\n",
    "            complex_train[curr_slot] = I_train[n,start_idx:end_idx] + 1j*Q_train[n,start_idx:end_idx]\n",
    "            if n < N_TEST:\n",
    "                complex_test[curr_slot] = I_test[n,start_idx:end_idx] + 1j*Q_test[n,start_idx:end_idx]\n",
    "\n",
    "    print(\"Expanded complex-valued training data has shape:\", complex_train.shape)\n",
    "    print(\"Expanded complex-valued testing data has shape:\", complex_test.shape)\n",
    "\n",
    "    # Verify construction\n",
    "    for n in range(N_TRAIN):\n",
    "        if N_PARTITIONS == 16:\n",
    "            X1 = np.concatenate((complex_train[16*n],complex_train[16*n+1],complex_train[16*n+2],complex_train[16*n+3],\n",
    "                                 complex_train[16*n+4],complex_train[16*n+5],complex_train[16*n+6],complex_train[16*n+7]))\n",
    "            X2 = np.concatenate((complex_train[16*n+8],complex_train[16*n+9],complex_train[16*n+10],complex_train[16*n+11],\n",
    "                                 complex_train[16*n+12],complex_train[16*n+13],complex_train[16*n+14],complex_train[16*n+15]))\n",
    "            X = np.concatenate((X1, X2))\n",
    "        elif N_PARTITIONS == 8:\n",
    "            X1 = np.concatenate((complex_train[8*n],complex_train[8*n+1],complex_train[8*n+2],complex_train[8*n+3]))\n",
    "            X2 = np.concatenate((complex_train[8*n+4],complex_train[8*n+5],complex_train[8*n+6],complex_train[8*n+7]))\n",
    "            X = np.concatenate((X1, X2))\n",
    "        elif N_PARTITIONS == 4:\n",
    "            X = np.concatenate((complex_train[4*n],complex_train[4*n+1],complex_train[4*n+2],complex_train[4*n+3]))\n",
    "        elif N_PARTITIONS == 2:\n",
    "            X = np.concatenate((complex_train[2*n],complex_train[2*n+1]))\n",
    "        else:\n",
    "            raise ValueError(\"Partition size must be 1, 2, 4, or 8\")\n",
    "        X_prime = I_train[n] + 1j*Q_train[n]\n",
    "        if not np.array_equal(X, X_prime):\n",
    "            raise ValueError(\"Construction failed\")\n",
    "\n",
    "df = pd.DataFrame(complex_train)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN_EX = complex_train.shape[0]\n",
    "N_TEST_EX = complex_test.shape[0]\n",
    "N_FEATURES = 4\n",
    "\n",
    "X_train = np.zeros((N_TRAIN_EX, N_SAMPLES, N_FEATURES))\n",
    "X_test = np.zeros((N_TEST_EX, N_SAMPLES, N_FEATURES))\n",
    "for dataset_len in [N_TRAIN_EX, N_TEST_EX]:\n",
    "    for i in range(dataset_len):\n",
    "        if dataset_len == N_TRAIN_EX:\n",
    "            I = np.real(complex_train)[i]\n",
    "            Q = np.imag(complex_train)[i]\n",
    "        else:\n",
    "            I = np.real(complex_test)[i]\n",
    "            Q = np.imag(complex_test)[i]\n",
    "        phase = np.arctan2(Q, I)\n",
    "        amp = np.sqrt(Q**2 + I**2)\n",
    "        if dataset_len == N_TRAIN_EX:\n",
    "            X_train[i,:,0] = I\n",
    "            X_train[i,:,1] = Q\n",
    "            X_train[i,:,2] = phase\n",
    "            X_train[i,:,3] = amp\n",
    "        else:\n",
    "            X_test[i,:,0] = I\n",
    "            X_test[i,:,1] = Q\n",
    "            X_test[i,:,2] = phase\n",
    "            X_test[i,:,3] = amp\n",
    "\n",
    "train_shape = X_train.shape\n",
    "test_shape = X_test.shape\n",
    "\n",
    "print(\"Training feature tensor has shape:\", X_train.shape)\n",
    "print(\"Testing feature tensor has shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define convolutional neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Activation, Flatten, BatchNormalization\n",
    "\n",
    "def cnn(INPUT_SIZE, N_CLASSES):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, input_shape=INPUT_SIZE))\n",
    "    model.add(Activation(\"selu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(64, 3, input_shape=INPUT_SIZE))\n",
    "    model.add(Activation(\"selu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(64, 3, input_shape=INPUT_SIZE))\n",
    "    model.add(Activation(\"selu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(64, 3, input_shape=INPUT_SIZE))\n",
    "    model.add(Activation(\"selu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(64, 3, input_shape=INPUT_SIZE))\n",
    "    model.add(Activation(\"selu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"selu\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation=\"selu\"))\n",
    "    model.add(Dense(N_CLASSES, activation=\"softmax\"))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "BOOTSTRAP = True\n",
    "N_CLASSES = 10\n",
    "N_EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "if BOOTSTRAP:\n",
    "    N_BOOTSTRAPS = 10\n",
    "    SAMPLE_SIZE = int(0.9*N_TRAIN_EX)\n",
    "    bootstrap_idxs = np.zeros((N_BOOTSTRAPS, SAMPLE_SIZE)).astype(int)\n",
    "    for i in range(N_BOOTSTRAPS):\n",
    "        bootstrap_idxs[i] = np.random.randint(low=0, high=N_TRAIN_EX, size=SAMPLE_SIZE)\n",
    "else:\n",
    "    N_BOOTSTRAPS = 1\n",
    "model_names = [\"cnn\"]\n",
    "models = []\n",
    "model_shapes = []\n",
    "for j in range(N_BOOTSTRAPS):\n",
    "    if BOOTSTRAP:\n",
    "        X_train_subset = X_train[bootstrap_idxs[j]]\n",
    "        y_train_subset = y_train[bootstrap_idxs[j]]\n",
    "    else:\n",
    "        X_train_subset = X_train\n",
    "        y_train_subset = y_train\n",
    "\n",
    "    X, X_val, y, y_val = train_test_split(X_train_subset, y_train_subset,\n",
    "                                          test_size=0.05, shuffle=True, random_state=0)\n",
    "\n",
    "    INPUT_SIZE = X.shape[1:]\n",
    "    model_shapes.append(INPUT_SIZE)\n",
    "    name = model_names[0]\n",
    "    y_1H = keras.utils.to_categorical(y, num_classes=N_CLASSES)\n",
    "    y_val_1H = keras.utils.to_categorical(y_val, num_classes=N_CLASSES)\n",
    "\n",
    "    model = cnn(INPUT_SIZE, N_CLASSES)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.0025), metrics=[\"accuracy\"])\n",
    "    model.fit(X, y_1H, epochs=N_EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val_1H))\n",
    "    model.save(\"./models/\" + name + \"_\" + str(j) + \".hdf5\")\n",
    "    models.append(model)\n",
    "    print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ensemble of CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import average, Input\n",
    "\n",
    "def ensemble_model(models, model_inputs):\n",
    "    outputs = [model(model_inputs[m]) for m,model in enumerate(models)]\n",
    "    avg = average(outputs) \n",
    "    ensemble = Model(inputs=model_inputs, outputs=avg, name=\"cnn-ensemble\")  \n",
    "    return ensemble\n",
    "\n",
    "repeated_model_names = np.repeat(model_names, len(models))\n",
    "model_inputs = [Input(shape=model_shapes[i], name=repeated_model_names[i]+ \"_\" + str(i)) for i in range(len(repeated_model_names))]\n",
    "ensemble = ensemble_model(models, model_inputs)\n",
    "print(ensemble.summary())\n",
    "ensemble.save(\"./models/cnn-ensemble.hdf5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test data by aggregating predictions on partitions of original signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble\n",
    "preds = model.predict([X_test]*len(models))           \n",
    "y_preds = np.ndarray(N_TEST)\n",
    "for n in range(N_TEST):\n",
    "    start_idx = N_PARTITIONS*n\n",
    "    end_idx = N_PARTITIONS*n+N_PARTITIONS\n",
    "    indep_probs = preds[start_idx]\n",
    "    for i in range(start_idx+1, end_idx):\n",
    "        indep_probs *= preds[i]\n",
    "    pred = np.argmax(indep_probs)\n",
    "    y_preds[n] = pred\n",
    "\n",
    "csv_data = [[\"Id\", \"Category\"]]\n",
    "for i in range(y_preds.shape[0]):\n",
    "    csv_data.append([str(i), int_to_label[y_preds[i]]])\n",
    "csv_df = pd.DataFrame(csv_data)\n",
    "csv_df.to_csv(\"cnn-ensemble.txt\", index=False, sep=\",\", header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
